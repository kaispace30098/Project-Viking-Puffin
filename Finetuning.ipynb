{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNeGdp458sjim+zu3nWVcE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaispace30098/Project-Viking-Puffin/blob/main/Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training a Custom Character LoRA with DreamBooth and SDXL**\n",
        "This notebook walks through the end-to-end process of fine-tuning the Stable Diffusion XL 1.0 model to recognize a new, custom character. We use a combination of DreamBooth and Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning."
      ],
      "metadata": {
        "id": "6KlcbRsNKjz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive to access our images and save the output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MEdVvVw9KrGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Prepare the Training Data\n",
        "Before running this notebook, the training data must be prepared.\n",
        "\n",
        "1.Collect Images: Gather 15-20 photos of your subject.\n",
        "\n",
        "2.Preprocess: Crop and resize these images to a square aspect ratio. While the model trains at 1024x1024, preprocessing them to a consistent size like 1024x1024 or 512x512 is recommended. I used the free online tool BIRME for this.\n",
        "\n",
        "3.Upload: Place the processed images in a folder on your Google Drive. The path to this folder will be used in the training command."
      ],
      "metadata": {
        "id": "abiJUs-HK2mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Run this code to verify the size of your preprocessed images\n",
        "# import os\n",
        "# from PIL import Image\n",
        "#\n",
        "# img_dir = \"/content/drive/MyDrive/MyLoRA/character_images\"\n",
        "# for filename in os.listdir(img_dir):\n",
        "#     if filename.lower().endswith((\".jpg\", \".png\", \"jpeg\")):\n",
        "#         path = os.path.join(img_dir, filename)\n",
        "#         with Image.open(path) as img:\n",
        "#             print(f\"{filename}: {img.size}\")"
      ],
      "metadata": {
        "id": "iK3I3cHNK9dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Environment Setup\n",
        "Here, we install all the necessary Python libraries.\n",
        "\n",
        "Note: I am installing diffusers directly from its GitHub source. This is crucial because the training script from the repository's main branch often requires features not yet available in the stable pip release, preventing version mismatch errors."
      ],
      "metadata": {
        "id": "5awBA2fkLKUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "print(\"Installing libraries...\")\n",
        "!pip install -q transformers accelerate bitsandbytes peft safetensors xformers\n",
        "# Install diffusers from the main branch on GitHub\n",
        "!pip install -q git+https://github.com/huggingface/diffusers.git\n",
        "print(\"Libraries installed successfully.\")"
      ],
      "metadata": {
        "id": "wzxtT1vPLOPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Configure Environment & Clone Repository\n",
        "First, we configure accelerate, which is a library from Hugging Face that simplifies running PyTorch scripts on any infrastructure (like Colab's single GPU).\n",
        "\n",
        "Second, we clone the diffusers repository to get access to the train_dreambooth_lora_sdxl.py training script."
      ],
      "metadata": {
        "id": "OsVL-lrULZO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Accelerate for Colab's single-GPU environment\n",
        "!accelerate config default\n",
        "print(\"Accelerate configured.\")\n",
        "\n",
        "# Clone the diffusers repository to get the training script\n",
        "import os\n",
        "if not os.path.exists('/content/diffusers'):\n",
        "  print(\"Cloning diffusers repository...\")\n",
        "  !git clone https://github.com/huggingface/diffusers.git\n",
        "else:\n",
        "  print(\"Diffusers repository already exists.\")"
      ],
      "metadata": {
        "id": "LWBh3HlrLh6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4: Define Training Parameters\n",
        "We use environment variables to keep the main training command clean and easy to read.\n",
        "\n",
        "MODEL_NAME: The base model we are fine-tuning (SDXL 1.0).\n",
        "\n",
        "INSTANCE_DIR: The path to our folder of training images on Google Drive.\n",
        "\n",
        "OUTPUT_DIR: The path where the trained LoRA and checkpoints will be saved.\n",
        "\n",
        "INSTANCE_PROMPT: The unique prompt that will trigger our character. This prompt creates the association between a new \"word\" (Kaiffin) and our images."
      ],
      "metadata": {
        "id": "6q7Nf3OXLpPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define environment variables\n",
        "%env MODEL_NAME=stabilityai/stable-diffusion-xl-base-1.0\n",
        "%env INSTANCE_DIR=/content/drive/MyDrive/MyLoRA/character_images\n",
        "%env OUTPUT_DIR=/content/drive/MyDrive/MyLoRA/output_lora\n",
        "%env INSTANCE_PROMPT=a photo of Kaiffin"
      ],
      "metadata": {
        "id": "ocjdqzcCMJM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5: Launch Training\n",
        "\n",
        "This is the final step. We use accelerate launch to execute the training script with all our specified parameters. The training will run for 1000 steps and save a checkpoint every 500 steps."
      ],
      "metadata": {
        "id": "faQkVylqMPao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final corrected training command\n",
        "!accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --instance_data_dir=$INSTANCE_DIR \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=2 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=1000 \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --validation_epochs=50 \\\n",
        "  --validation_prompt=\"photo_of_Kaiffin_on_an_icy_cliff\" \\\n",
        "  --seed=\"0\" \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_checkpointing \\\n",
        "  --enable_xformers_memory_efficient_attention \\\n",
        "  --report_to=\"tensorboard\""
      ],
      "metadata": {
        "id": "EIdZkkJDMT_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}